{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. We will build a NER to recognize named entities from Twitter.\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
    "\n",
    "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import random\n",
    "\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "data_path = \"./data/tweeter\"\n",
    "path_to_logdir = './logdir'\n",
    "path_to_model = \"./models\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(path_to_logdir):\n",
    "    os.makedirs(path_to_logdir)\n",
    "    \n",
    "if not os.path.exists(path_to_model):\n",
    "    os.makedirs(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/- Load the Twitter Named Entity Recognition corpus\n",
    "\n",
    "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "## 1) Read data\n",
    "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "\n",
    "            if token.startswith('http://') or token.startswith(\"https://\"):\n",
    "                token = \"<URL>\"\n",
    "            if token.startswith(\"@\"):\n",
    "                token = \"<USR>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data(os.path.join(data_path, 'train.txt'))\n",
    "validation_tokens, validation_tags = read_data(os.path.join(data_path, 'validation.txt'))\n",
    "test_tokens, test_tags = read_data(os.path.join(data_path, 'test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare dictionnaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = {}\n",
    "    \n",
    "    index = 0\n",
    "    for special_token in special_tokens:\n",
    "        tok2idx[special_token] = index\n",
    "        idx2tok[index] = special_token\n",
    "        index += 1\n",
    "    \n",
    "   \n",
    "    for seq in tokens_or_tags:\n",
    "        for tok in seq:\n",
    "            if not tok in tok2idx:\n",
    "                tok2idx[tok] = index\n",
    "                idx2tok[index] = tok\n",
    "                index += 1\n",
    "    \n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "# create the mapping between tokens and ids for a sentence\n",
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* we can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create dataset and datalaoder\n",
    "\n",
    "We will creatr know dataset object and dataloader, that well enable us to load batches of tweets. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/- Create and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create the model, i.e. class **NERTagger**, our model is a simple one it has only tree ayers, an embedding layer that transforms the indexe into vectors, the second layer is an LSTM layer and the last layer is a Linear layer that maps the output of the LSTM layer to the required dimension, i.e. the number of all possible tags. All those layers are implemented in pytorch and can be used easily. We define also the **train_one_epoch** function, that takes into charge the training procedure, it consists in loading batches, computing the output of the batch, coumputing by comparing the predicted output with the true target, the gradients of the loss function are then computed using *.backward()*, and finally a gradient descent step is done using *.step()*, we also provide **test_model** function that take care of evaluating the model on the validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, n_tags):\n",
    "        super(NERTagger, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_tags = n_tags\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim,  self.n_tags)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        return tag_scores\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_one_epoch(model, train_tokens, train_tags, batch_size, criterion, optimizer, writer, epoch):\n",
    "    n_samples = len(train_tokens)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    for batch_id, (x_batch, y_batch, lengths) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        x_batch = torch.tensor(x_batch).long().to(device)\n",
    "        y_batch = torch.tensor(y_batch).long().to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = ner_tagger(x_batch).permute(0, 2, 1)\n",
    "    \n",
    "        loss = criterion(outputs, y_batch)\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        y_pred = outputs.data.max(1)[1]\n",
    "    \n",
    "        correct += y_pred.eq(y_batch.data).cpu().sum() / y_batch.shape[1]\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        if batch_id % 100 == 99:\n",
    "            writer.add_scalar('training loss',\n",
    "                              running_loss / 100,\n",
    "                              epoch * (n_samples // batch_size) + batch_id)\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_id * x_batch.shape[0], n_samples,\n",
    "                       100. * batch_id / (n_samples // batch_size), running_loss / 100))\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            writer.flush()\n",
    "\n",
    "    writer.add_scalar('Train/Loss', loss.item(), epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_model(model, tokens, tags, criterion, writer=None, device=None, epoch=None):\n",
    "    model.eval()\n",
    "    i, loss, correct, n = [0, 0, 0, 0]\n",
    "    n_samples = 0\n",
    "\n",
    "    print(\"Testing..\")\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (x_batch, y_batch, lengths) in enumerate(batches_generator(batch_size, tokens, tags)):\n",
    "            x_batch = torch.tensor(x_batch).long().to(device)\n",
    "            y_batch = torch.tensor(y_batch).long().to(device)\n",
    "\n",
    "            outputs = model(x_batch).permute(0, 2, 1)\n",
    "\n",
    "            loss += criterion(outputs, y_batch)\n",
    "\n",
    "            y_pred = outputs.data.max(1)[1]\n",
    "            correct += y_pred.eq(y_batch.data).cpu().sum() / y_batch.shape[1]\n",
    "            n += 1\n",
    "            n_samples += int(x_batch.shape[0])\n",
    "\n",
    "    loss /= n  # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / (n_samples)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, correct, n_samples,\n",
    "        accuracy))\n",
    "\n",
    "    if writer:\n",
    "        # Record loss and accuracy into the writer\n",
    "        writer.add_scalar('Test/Loss', loss, epoch)\n",
    "        writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "        writer.flush()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_tokens, train_tags, validation_tokens, validation_tags, batch_size, criterion, optimizer, writer, n_epochs=10):\n",
    "    best_acc = 0.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        train_one_epoch(model, train_tokens, train_tags, batch_size, criterion, optimizer, writer, epoch)\n",
    "        acc = test_model(model, validation_tokens, validation_tags, criterion, writer, device, epoch)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model, os.path.join(path_to_model, \"ner_best.pth\"))\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train now our model, note that tensorboard can be used to view the evolution of the training processus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3540), started 0:01:40 ago. (Use '!kill 3540' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e9bf815fc419e6a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e9bf815fc419e6a5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "PATH_to_log_dir = './logdir'\n",
    "if not os.path.exists(PATH_to_log_dir):\n",
    "    os.makedirs(PATH_to_log_dir)\n",
    "    \n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(os.path.join(path_to_logdir, timestr))\n",
    "    \n",
    "%tensorboard --logdir {PATH_to_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Epoch: 0 [3168/5795 (55%)]\tLoss: 0.500505\n",
      "Testing..\n",
      "\n",
      "Test set: Average loss: 0.2546, Accuracy: 682/724 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type NERTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Epoch: 1 [3168/5795 (55%)]\tLoss: 0.255444\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 2\n",
    "dropout_keep_probability = 0.5\n",
    "hidden_dim = 512\n",
    "embedding_dim = 128\n",
    "\n",
    "\n",
    "ner_tagger = NERTagger(embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                       vocab_size=len(token2idx), n_tags=len(tag2idx))\n",
    "\n",
    "ner_tagger = ner_tagger.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(ner_tagger.parameters())\n",
    "\n",
    "\n",
    "\n",
    "train_model(ner_tagger, train_tokens, train_tags, validation_tokens,\n",
    "            validation_tags, batch_size, criterion, optimizer,\n",
    "            writer, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we give the performence of the Neural NER tagger on the test set. Furthermore, we give some examples of the application of the trained on sentences from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_model(ner_tagger, test_tokens, test_tags, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that our model gives good results, with approximately 95% of coorect tags. This result can even be improved by using more complex architectures and by spending more time on fine-tuining the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give now some examples of usage of the traned NER tagger on different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, tags=None, verbosity=0):\n",
    "    X = torch.tensor(words2idxs(sentence)).to(device).unsqueeze(0)\n",
    "    \n",
    "    y_pred = ner_tagger(X)\n",
    "    y_pred = y_pred.squeeze().max(1)[1].cpu()\n",
    "    \n",
    "    res = list(y_pred.numpy())\n",
    "    res = idxs2tags(res)\n",
    "\n",
    "    if tags:\n",
    "        y_true = torch.tensor(tags2idxs(true_tags))\n",
    "        correct = y_pred.eq(y_true.data).cpu().sum()\n",
    "        if verbosity:\n",
    "            print(\"Correct tags {}/{}\".format(int(correct), len(res)))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger.eval()\n",
    "\n",
    "\n",
    "indexes = random.sample(range(1, len(test_tokens)), 10)\n",
    "\n",
    "for idx in indexes:\n",
    "    sentence = test_tokens[idx]\n",
    "    true_tags = test_tags[idx]\n",
    "    print(\"We consider\")\n",
    "    print(\"Input sentence: \", ' '.join(sentence))\n",
    "    print(\"Predicted NER tag\", ' '.join(predict_sentence(sentence, tags=true_tags, verbosity=1)))\n",
    "    print(\"*\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
